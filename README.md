# Pipeline de ExtraÃ§Ã£o de Dados - Airflow

Este projeto implementa um pipeline de extraÃ§Ã£o de dados utilizando Apache Airflow, Docker e PostgreSQL para automatizar a coleta de dados de mÃºltiplas fontes e carregÃ¡-los em um Data Warehouse.

## ğŸ“‹ VisÃ£o Geral

O pipeline realiza extraÃ§Ã£o de dados de:
- Banco de dados PostgreSQL (origem)
- Arquivo CSV com transaÃ§Ãµes de clientes

Os dados extraÃ­dos sÃ£o processados e carregados em um Data Warehouse containerizado para anÃ¡lise posterior.

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚    â”‚     CSV File    â”‚    â”‚   Data Lake     â”‚
â”‚   (Origem)      â”‚â”€â”€â”€â–¶â”‚   (TransaÃ§Ãµes)  â”‚â”€â”€â”€â–¶â”‚   (Storage)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚  Data Warehouse â”‚
                                              â”‚   (PostgreSQL)  â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tecnologias Utilizadas

- **Apache Airflow** - OrquestraÃ§Ã£o de workflows
- **Astro CLI** - Desenvolvimento local do Airflow
- **Docker & Docker Compose** - ContainerizaÃ§Ã£o
- **PostgreSQL** - Banco de dados origem e Data Warehouse
- **Pandas** - Processamento de dados
- **Python 3.12** - Linguagem principal

## ğŸ“ Estrutura do Projeto

```
project/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pipeline_extraction.py    # DAG principal
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ transacoes.csv           # Arquivo CSV de origem
â”‚   â””â”€â”€ datalake/               # DiretÃ³rio de armazenamento temporÃ¡rio
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš™ï¸ ConfiguraÃ§Ã£o

### PrÃ©-requisitos

- Docker e Docker Compose instalados
- Astro CLI instalado (`pip install astro-cli`)
- Python 3.8+

### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**
```bash
git clone https://github.com/luizfernandoOliveiraa/Airflow_Project.git
cd pipeline-extraction
```

2. **Inicie o ambiente com Astro**
```bash
astro dev start
```

3. **Configure as conexÃµes no Airflow**

Acesse `http://localhost:8080` e configure as seguintes conexÃµes:

#### ConexÃ£o: `db_origem`
- **Connection Type**: Postgres
- **Host**: db_origem 
- **Schema**: db_origem
- **Login**: dborigem_teste
- **Password**: dborigem_teste2025
- **Port**: 5432

#### ConexÃ£o: `db_datawarehouse`
- **Connection Type**: Postgres
- **Host**: db_datawarehouse
- **Schema**: db_datawarehouse
- **Login**: db_datawarehouse_teste2025
- **Password**: db_datawarehouse_teste2025
- **Port**: 5432

## ğŸš€ ExecuÃ§Ã£o

### ExecuÃ§Ã£o Manual
1. Acesse a interface do Airflow em `http://localhost:8080`
2. Localize a DAG `pipeline_extraction`
3. Ative a DAG e execute manualmente ou aguarde o agendamento

### ExecuÃ§Ã£o AutomÃ¡tica
A DAG estÃ¡ configurada para executar diariamente Ã s 4:35 AM:
```python
schedule='35 4 * * *'
```

## ğŸ“Š Tabelas ExtraÃ­das

### Do PostgreSQL:
- `agencias`
- `clientes`
- `colaborador_agencia`
- `colaboradores`
- `contas`
- `propostas_credito`

### Do CSV:
- `transacoes` (dados de transaÃ§Ãµes de clientes)

## ğŸ”„ Fluxo de ExecuÃ§Ã£o

1. **ExtraÃ§Ã£o PostgreSQL** (`postgres_extract`)
   - Conecta no banco de origem
   - Extrai dados das tabelas configuradas
   - Salva no data lake como CSV

2. **ExtraÃ§Ã£o CSV** (`extract_csv`)
   - LÃª arquivo de transaÃ§Ãµes
   - Processa e salva no data lake

3. **Carregamento Data Warehouse** (`load_dw`)
   - Carrega todos os CSVs do data lake
   - Insere/atualiza tabelas no DW
   - Limpa dados temporÃ¡rios

## ğŸ“ Logs e Monitoramento

- Logs detalhados disponÃ­veis na interface do Airflow
- Tratamento de erros implementado em todas as tasks
- NotificaÃ§Ãµes de falha configurÃ¡veis

## ğŸ”§ PersonalizaÃ§Ã£o

### Adicionar Nova Tabela
1. Adicione o nome da tabela em `SOURCE_TABLES`
2. Reinicie a DAG

### Modificar Agendamento
Altere o parÃ¢metro `schedule` na definiÃ§Ã£o da DAG:
```python
@dag(
    dag_id="pipeline_extraction",
    schedule='0 6 * * *',  # 6:00 AM diÃ¡rio
    ...
)
```

## ğŸ› Troubleshooting

### Problemas Comuns

**Erro de ConexÃ£o com Banco**
- Verifique se as conexÃµes estÃ£o configuradas corretamente
- Confirme se os containers estÃ£o rodando

**Arquivo CSV nÃ£o encontrado**
- Verifique se o arquivo estÃ¡ em `/usr/local/airflow/include/transacoes.csv`
- Confirme as permissÃµes de leitura

**Falha no Carregamento do DW**
- Verifique espaÃ§o em disco
- Confirme credenciais do Data Warehouse

### Comandos Ãšteis

```bash
# Ver logs dos containers
docker-compose logs -f

# Reiniciar ambiente Airflow
astro dev restart

# Acessar container Airflow
astro dev bash

# Parar ambiente
astro dev stop
```

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/NovaFuncionalidade`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/NovaFuncionalidade`)
5. Abra um Pull Request

Obs: Esse projeto foi desenvolvido em outro ambiente restrito, e adaptado para esse repositÃ³rio, por isso temos poucos commits e sem desenvolvimento de testes.
